{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#https://jessesw.com/Data-Science-Skills/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup # For HTML parsing\n",
    "import urllib2 # Website connections\n",
    "import re # Regular expressions\n",
    "from time import sleep # To prevent overwhelming the server between connections\n",
    "from collections import Counter # Keep track of our term counts\n",
    "from nltk.corpus import stopwords # Filter out stopwords, such as 'the', 'or', 'and'\n",
    "import pandas as pd # For converting results to a dataframe and bar chart plots\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_cleaner(website):\n",
    "    '''\n",
    "    This function just cleans up the raw html so that I can look at it.\n",
    "    Inputs: a URL to investigate\n",
    "    Outputs: Cleaned text only\n",
    "    '''\n",
    "    try:\n",
    "        site = urllib2.urlopen(website).read() # Connect to the job posting\n",
    "    except: \n",
    "        return   # Need this in case the website isn't there anymore or some other weird connection problem \n",
    "    \n",
    "    soup_obj = BeautifulSoup(site) # Get the html from the site\n",
    "    \n",
    "    for script in soup_obj([\"script\", \"style\"]):\n",
    "        script.extract() # Remove these two elements from the BS4 object\n",
    "    \n",
    "    \n",
    "\n",
    "    text = soup_obj.get_text() # Get the text from this\n",
    "    \n",
    "        \n",
    "    \n",
    "    lines = (line.strip() for line in text.splitlines()) # break into lines\n",
    "    \n",
    "        \n",
    "        \n",
    "    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \")) # break multi-headlines into a line each\n",
    "    \n",
    "    def chunk_space(chunk):\n",
    "        chunk_out = chunk + ' ' # Need to fix spacing issue\n",
    "        return chunk_out  \n",
    "        \n",
    "    \n",
    "    text = ''.join(chunk_space(chunk) for chunk in chunks if chunk).encode('utf-8') # Get rid of all blank lines and ends of line\n",
    "        \n",
    "        \n",
    "    # Now clean out all of the unicode junk (this line works great!!!)\n",
    "        \n",
    "    try:\n",
    "        text = text.decode('unicode_escape').encode('ascii', 'ignore') # Need this as some websites aren't formatted\n",
    "    except:                                                            # in a way that this works, can occasionally throw\n",
    "        return                                                         # an exception\n",
    "       \n",
    "        \n",
    "    text = re.sub(\"[^a-zA-Z.+3]\",\" \", text)  # Now get rid of any terms that aren't words (include 3 for d3.js)\n",
    "                                                # Also include + for C++\n",
    "        \n",
    "       \n",
    "    text = text.lower().split()  # Go to lower case and split them apart\n",
    "        \n",
    "        \n",
    "    stop_words = set(stopwords.words(\"english\")) # Filter out any stop words\n",
    "    text = [w for w in text if not w in stop_words]\n",
    "        \n",
    "        \n",
    "        \n",
    "    text = list(set(text)) # Last, just get the set of these. Ignore counts (we are just looking at whether a term existed\n",
    "                            # or not on the website)\n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_cleaner(website):\n",
    "    '''\n",
    "    This function just cleans up the raw html so that I can look at it.\n",
    "    Inputs: a URL to investigate\n",
    "    Outputs: Cleaned text only\n",
    "    '''\n",
    "    try:\n",
    "        site = urllib2.urlopen(website).read() # Connect to the job posting\n",
    "    except: \n",
    "        return   # Need this in case the website isn't there anymore or some other weird connection problem \n",
    "    \n",
    "    soup_obj = BeautifulSoup(site) # Get the html from the site\n",
    "    \n",
    "    for script in soup_obj([\"script\", \"style\"]):\n",
    "        script.extract() # Remove these two elements from the BS4 object\n",
    "    \n",
    "    \n",
    "\n",
    "    text = soup_obj.get_text() # Get the text from this\n",
    "    \n",
    "        \n",
    "    \n",
    "    lines = (line.strip() for line in text.splitlines()) # break into lines\n",
    "    \n",
    "        \n",
    "        \n",
    "    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \")) # break multi-headlines into a line each\n",
    "    \n",
    "    def chunk_space(chunk):\n",
    "        chunk_out = chunk + ' ' # Need to fix spacing issue\n",
    "        return chunk_out  \n",
    "        \n",
    "    \n",
    "    text = ''.join(chunk_space(chunk) for chunk in chunks if chunk).encode('utf-8') # Get rid of all blank lines and ends of line\n",
    "        \n",
    "        \n",
    "    # Now clean out all of the unicode junk (this line works great!!!)\n",
    "        \n",
    "    try:\n",
    "        text = text.decode('unicode_escape').encode('ascii', 'ignore') # Need this as some websites aren't formatted\n",
    "    except:                                                            # in a way that this works, can occasionally throw\n",
    "        return                                                         # an exception\n",
    "       \n",
    "        \n",
    "    text = re.sub(\"[^a-zA-Z.+3]\",\" \", text)  # Now get rid of any terms that aren't words (include 3 for d3.js)\n",
    "                                                # Also include + for C++\n",
    "        \n",
    "       \n",
    "    text = text.lower().split()  # Go to lower case and split them apart\n",
    "        \n",
    "        \n",
    "    stop_words = set(stopwords.words(\"english\")) # Filter out any stop words\n",
    "    text = [w for w in text if not w in stop_words]\n",
    "        \n",
    "        \n",
    "        \n",
    "    text = list(set(text)) # Last, just get the set of these. Ignore counts (we are just looking at whether a term existed\n",
    "                            # or not on the website)\n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['roi',\n",
       " 'help',\n",
       " 'skip',\n",
       " 'searchclose',\n",
       " 'competitive',\n",
       " 'trade',\n",
       " 'customer',\n",
       " 'relationships.to',\n",
       " 'interpersonal',\n",
       " 'including',\n",
       " 'human',\n",
       " 'research.manage',\n",
       " 'keywords',\n",
       " 'contractsalary',\n",
       " 'benefit.consolidate',\n",
       " 'find',\n",
       " 'wireframes',\n",
       " 'impact',\n",
       " 'content',\n",
       " 'like']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = text_cleaner('https://www.indeed.com/job/ux-researcher-0e613ff3bce04c82')\n",
    "sample[:20] # Just show the first 20 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def skills_info(city = None, state = None):\n",
    "    '''\n",
    "    This function will take a desired city/state and look for all new job postings\n",
    "    on Indeed.com. It will crawl all of the job postings and keep track of how many\n",
    "    use a preset list of typical data science skills. The final percentage for each skill\n",
    "    is then displayed at the end of the collation. \n",
    "        \n",
    "    Inputs: The location's city and state. These are optional. If no city/state is input, \n",
    "    the function will assume a national search (this can take a while!!!).\n",
    "    Input the city/state as strings, such as skills_info('Chicago', 'IL').\n",
    "    Use a two letter abbreviation for the state.\n",
    "    \n",
    "    https://www.indeed.com/jobs?q=data+scientist%22&l=san+francisco,+CA  \n",
    "    \n",
    "    Output: A bar chart showing the most commonly desired skills in the job market for \n",
    "    a data scientist. \n",
    "    '''      \n",
    "    \n",
    "    \n",
    "    final_job = 'python' # searching for data scientist exact fit(\"data scientist\" on Indeed search)\n",
    "    \n",
    "    # Make sure the city specified works properly if it has more than one word (such as San Francisco)\n",
    "    if city is not None:\n",
    "        final_city = city.split() \n",
    "        final_city = '+'.join(word for word in final_city)\n",
    "        final_site_list = ['http://www.indeed.com/jobs?q=%22', final_job, '%22&l=', final_city,\n",
    "                    '%2C+', state] # Join all of our strings together so that indeed will search correctly\n",
    "    else:\n",
    "        final_site_list = ['http://www.indeed.com/jobs?q=\"', final_job, '\"']\n",
    "\n",
    "    final_site = ''.join(final_site_list) # Merge the html address together into one string\n",
    "\n",
    "    \n",
    "    base_url = 'http://www.indeed.com'\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        html = urllib2.urlopen(final_site).read() # Open up the front page of our search first\n",
    "    except:\n",
    "        'That city/state combination did not have any jobs. Exiting . . .' # In case the city is invalid\n",
    "        return\n",
    "    soup = BeautifulSoup(html) # Get the html from the first page\n",
    "    \n",
    "    # Now find out how many jobs there were\n",
    "    \n",
    "    num_jobs_area = soup.find(id = 'searchCount').string.encode('utf-8') # Now extract the total number of jobs found\n",
    "                                                                        # The 'searchCount' object has this\n",
    "    \n",
    "    job_numbers = re.findall('\\d+', num_jobs_area) # Extract the total jobs found from the search result\n",
    "    \n",
    "    print(job_numbers)\n",
    "    if len(job_numbers) > 3: # Have a total number of jobs greater than 1000\n",
    "        total_num_jobs = (int(job_numbers[2])*1000) + int(job_numbers[3])\n",
    "    else:\n",
    "        total_num_jobs = int(job_numbers[2]) \n",
    "    \n",
    "    city_title = city\n",
    "    if city is None:\n",
    "        city_title = 'Nationwide'\n",
    "        \n",
    "    print 'There were', total_num_jobs, 'jobs found,', city_title # Display how many jobs were found\n",
    "    \n",
    "    num_pages = total_num_jobs/10 # This will be how we know the number of times we need to iterate over each new\n",
    "                                      # search result page\n",
    "    job_descriptions = [] # Store all our descriptions in this list\n",
    "    \n",
    "    for i in xrange(1,num_pages+1): # Loop through all of our search result pages\n",
    "        print 'Getting page', i\n",
    "        start_num = str(i*10) # Assign the multiplier of 10 to view the pages we want\n",
    "        current_page = ''.join([final_site, '&start=', start_num])\n",
    "        # Now that we can view the correct 10 job returns, start collecting the text samples from each\n",
    "            \n",
    "        html_page = urllib2.urlopen(current_page).read() # Get the page\n",
    "        \n",
    "        \n",
    "        page_obj = BeautifulSoup(html_page) # Locate all of the job links\n",
    "        job_link_area = page_obj.find(id = 'resultsCol') # The center column on the page where the job postings exist\n",
    "\n",
    "        job_URLS = [base_url + link.get('href') for link in job_link_area.find_all('a')] # Get the URLS for the jobs\n",
    "            \n",
    "        job_URLS = filter(lambda x:'clk' in x, job_URLS) # Now get just the job related URLS\n",
    "            \n",
    "        \n",
    "        for j in xrange(0,len(job_URLS)):\n",
    "            final_description = text_cleaner(job_URLS[j])\n",
    "            if final_description: # So that we only append when the website was accessed correctly\n",
    "                job_descriptions.append(final_description)\n",
    "            sleep(1) # So that we don't be jerks. If you have a very fast internet connection you could hit the server a lot! \n",
    "        \n",
    "    print 'Done with collecting the job postings!'    \n",
    "    print 'There were', len(job_descriptions), 'jobs successfully found.'\n",
    "    \n",
    "    \n",
    "    doc_frequency = Counter() # This will create a full counter of our terms. \n",
    "    [doc_frequency.update(item) for item in job_descriptions] # List comp\n",
    "    \n",
    "    # Now we can just look at our final dict list inside doc_frequency\n",
    "    \n",
    "    # Obtain our key terms and store them in a dict. These are the key data science skills we are looking for\n",
    "    \n",
    "    prog_lang_dict = Counter({'R':doc_frequency['r'], 'Python':doc_frequency['python'],\n",
    "                    'Java':doc_frequency['java'], 'C++':doc_frequency['c++'],\n",
    "                    'Ruby':doc_frequency['ruby'],\n",
    "                    'Perl':doc_frequency['perl'], 'Matlab':doc_frequency['matlab'],\n",
    "                    'JavaScript':doc_frequency['javascript'], 'Scala': doc_frequency['scala']})\n",
    "                      \n",
    "    analysis_tool_dict = Counter({'Excel':doc_frequency['excel'],  'Tableau':doc_frequency['tableau'],\n",
    "                        'D3.js':doc_frequency['d3.js'], 'SAS':doc_frequency['sas'],\n",
    "                        'SPSS':doc_frequency['spss'], 'D3':doc_frequency['d3']})  \n",
    "\n",
    "    hadoop_dict = Counter({'Hadoop':doc_frequency['hadoop'], 'MapReduce':doc_frequency['mapreduce'],\n",
    "                'Spark':doc_frequency['spark'], 'Pig':doc_frequency['pig'],\n",
    "                'Hive':doc_frequency['hive'], 'Shark':doc_frequency['shark'],\n",
    "                'Oozie':doc_frequency['oozie'], 'ZooKeeper':doc_frequency['zookeeper'],\n",
    "                'Flume':doc_frequency['flume'], 'Mahout':doc_frequency['mahout']})\n",
    "                \n",
    "    database_dict = Counter({'SQL':doc_frequency['sql'], 'NoSQL':doc_frequency['nosql'],\n",
    "                    'HBase':doc_frequency['hbase'], 'Cassandra':doc_frequency['cassandra'],\n",
    "                    'MongoDB':doc_frequency['mongodb']})\n",
    "                     \n",
    "               \n",
    "    overall_total_skills = prog_lang_dict + analysis_tool_dict + hadoop_dict + database_dict # Combine our Counter objects\n",
    "    \n",
    "        \n",
    "    \n",
    "    final_frame = pd.DataFrame(overall_total_skills.items(), columns = ['Term', 'NumPostings']) # Convert these terms to a \n",
    "                                                                                                # dataframe \n",
    "    \n",
    "    # Change the values to reflect a percentage of the postings \n",
    "    \n",
    "    final_frame.NumPostings = (final_frame.NumPostings)*100/len(job_descriptions) # Gives percentage of job postings \n",
    "                                                                                    #  having that term \n",
    "    \n",
    "    # Sort the data for plotting purposes\n",
    "    \n",
    "    final_frame.sort(columns = 'NumPostings', ascending = False, inplace = True)\n",
    "    \n",
    "    # Get it ready for a bar plot\n",
    "        \n",
    "    final_plot = final_frame.plot(x = 'Term', kind = 'bar', legend = None, \n",
    "                            title = 'Percentage of Data Scientist Job Ads with a Key Skill, ' + city_title)\n",
    "        \n",
    "    final_plot.set_ylabel('Percentage Appearing in Job Ads')\n",
    "    fig = final_plot.get_figure() # Have to convert the pandas plot object to a matplotlib object\n",
    "        \n",
    "        \n",
    "    return fig, final_frame # End of the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '3', '986']\n",
      "There were 986 jobs found, San Francisco\n",
      "Getting page 1\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot concatenate 'str' and 'NoneType' objects",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-63d4c6ba5e4e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msilicon_val_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mskills_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcity\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'San Francisco'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'CA'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-15-9158c40da38e>\u001b[0m in \u001b[0;36mskills_info\u001b[1;34m(city, state)\u001b[0m\n\u001b[0;32m     77\u001b[0m         \u001b[0mjob_link_area\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpage_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'resultsCol'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# The center column on the page where the job postings exist\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m         \u001b[0mjob_URLS\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbase_url\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mlink\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'href'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mlink\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mjob_link_area\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'a'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# Get the URLS for the jobs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m         \u001b[0mjob_URLS\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m'clk'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjob_URLS\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Now get just the job related URLS\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot concatenate 'str' and 'NoneType' objects"
     ]
    }
   ],
   "source": [
    "silicon_val_info = skills_info(city = 'San Francisco', state = 'CA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      "['1', '3', '984']\n",
      "There were 984 jobs found, San Francisco\n",
      "Done with collecting the job postings!\n",
      "There were 0 jobs successfully found.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'sort'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-7d084854f162>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[1;31m# Sort the data for plotting purposes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 142\u001b[1;33m \u001b[0mfinal_frame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'NumPostings'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mascending\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[1;31m# Get it ready for a bar plot\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda2\\lib\\site-packages\\pandas\\core\\generic.pyc\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   3079\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3080\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3081\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3082\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3083\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'sort'"
     ]
    }
   ],
   "source": [
    "city, state = \"San Francisco\", 'CA'\n",
    "'''\n",
    "This function will take a desired city/state and look for all new job postings\n",
    "on Indeed.com. It will crawl all of the job postings and keep track of how many\n",
    "use a preset list of typical data science skills. The final percentage for each skill\n",
    "is then displayed at the end of the collation. \n",
    "\n",
    "Inputs: The location's city and state. These are optional. If no city/state is input, \n",
    "the function will assume a national search (this can take a while!!!).\n",
    "Input the city/state as strings, such as skills_info('Chicago', 'IL').\n",
    "Use a two letter abbreviation for the state.\n",
    "\n",
    "https://www.indeed.com/jobs?q=data+scientist%22&l=san+francisco,+CA  \n",
    "\n",
    "Output: A bar chart showing the most commonly desired skills in the job market for \n",
    "a data scientist. \n",
    "'''      \n",
    "\n",
    "#data+scientist+\n",
    "final_job = 'python' # searching for data scientist exact fit(\"data scientist\" on Indeed search)\n",
    "\n",
    "# Make sure the city specified works properly if it has more than one word (such as San Francisco)\n",
    "if city is not None:\n",
    "    final_city = city.split() \n",
    "    final_city = '+'.join(word for word in final_city)\n",
    "    final_site_list = ['http://www.indeed.com/jobs?q=%22', final_job, '%22&l=', final_city,\n",
    "                '%2C+', state] # Join all of our strings together so that indeed will search correctly\n",
    "else:\n",
    "    final_site_list = ['http://www.indeed.com/jobs?q=\"', final_job, '\"']\n",
    "\n",
    "final_site = ''.join(final_site_list) # Merge the html address together into one string\n",
    "\n",
    "\n",
    "base_url = 'http://www.indeed.com'\n",
    "\n",
    "\n",
    "try:\n",
    "    html = urllib2.urlopen(final_site).read() # Open up the front page of our search first\n",
    "except:\n",
    "    'That city/state combination did not have any jobs. Exiting . . .' # In case the city is invalid\n",
    "#     return\n",
    "soup = BeautifulSoup(html) # Get the html from the first page\n",
    "\n",
    "# Now find out how many jobs there were\n",
    "\n",
    "num_jobs_area = soup.find(id = 'searchCount').string.encode('utf-8') # Now extract the total number of jobs found\n",
    "                                                                    # The 'searchCount' object has this\n",
    "\n",
    "job_numbers = re.findall('\\d+', num_jobs_area) # Extract the total jobs found from the search result\n",
    "\n",
    "print('hello')\n",
    "print(job_numbers)\n",
    "\n",
    "if len(job_numbers) > 3: # Have a total number of jobs greater than 1000\n",
    "    total_num_jobs = (int(job_numbers[2])*1000) + int(job_numbers[3])\n",
    "else:\n",
    "    total_num_jobs = int(job_numbers[2]) \n",
    "\n",
    "city_title = city\n",
    "if city is None:\n",
    "    city_title = 'Nationwide'\n",
    "\n",
    "print 'There were', total_num_jobs, 'jobs found,', city_title # Display how many jobs were found\n",
    "\n",
    "num_pages = total_num_jobs/10 # This will be how we know the number of times we need to iterate over each new\n",
    "                                  # search result page\n",
    "job_descriptions = [] # Store all our descriptions in this list\n",
    "\n",
    "for i in xrange(1,1):#num_pages+1): # Loop through all of our search result pages\n",
    "    print 'Getting page', i\n",
    "    start_num = str(i*10) # Assign the multiplier of 10 to view the pages we want\n",
    "    current_page = ''.join([final_site, '&start=', start_num])\n",
    "    # Now that we can view the correct 10 job returns, start collecting the text samples from each\n",
    "    sleep(5)\n",
    "    #print (current_page)\n",
    "    \n",
    "    html_page = urllib2.urlopen(current_page).read() # Get the page\n",
    "\n",
    "    page_obj = BeautifulSoup(html_page) # Locate all of the job links\n",
    "    job_link_area = page_obj.find(id = 'resultsCol') # The center column on the page where the job postings exist\n",
    "    #print (\"job_link_area\")\n",
    "    #print (job_link_area)\n",
    "    \n",
    "    print ((link.get('href') for link in job_link_area.find_all('a')))\n",
    "    job_URLS = [base_url + link.get('href') for link in job_link_area.find_all('a')] # Get the URLS for the jobs\n",
    "\n",
    "    job_URLS = filter(lambda x:'clk' in x, job_URLS) # Now get just the job related URLS\n",
    "    print job_URLS\n",
    "\n",
    "    for j in xrange(0,len(job_URLS)):\n",
    "        final_description = text_cleaner(job_URLS[j])\n",
    "        if final_description: # So that we only append when the website was accessed correctly\n",
    "            job_descriptions.append(final_description)\n",
    "        sleep(1) # So that we don't be jerks. If you have a very fast internet connection you could hit the server a lot! \n",
    "\n",
    "print 'Done with collecting the job postings!'    \n",
    "print 'There were', len(job_descriptions), 'jobs successfully found.'\n",
    "\n",
    "\n",
    "doc_frequency = Counter() # This will create a full counter of our terms. \n",
    "[doc_frequency.update(item) for item in job_descriptions] # List comp\n",
    "\n",
    "# Now we can just look at our final dict list inside doc_frequency\n",
    "\n",
    "# Obtain our key terms and store them in a dict. These are the key data science skills we are looking for\n",
    "\n",
    "prog_lang_dict = Counter({'R':doc_frequency['r'], 'Python':doc_frequency['python'],\n",
    "                'Java':doc_frequency['java'], 'C++':doc_frequency['c++'],\n",
    "                'Ruby':doc_frequency['ruby'],\n",
    "                'Perl':doc_frequency['perl'], 'Matlab':doc_frequency['matlab'],\n",
    "                'JavaScript':doc_frequency['javascript'], 'Scala': doc_frequency['scala']})\n",
    "\n",
    "analysis_tool_dict = Counter({'Excel':doc_frequency['excel'],  'Tableau':doc_frequency['tableau'],\n",
    "                    'D3.js':doc_frequency['d3.js'], 'SAS':doc_frequency['sas'],\n",
    "                    'SPSS':doc_frequency['spss'], 'D3':doc_frequency['d3']})  \n",
    "\n",
    "hadoop_dict = Counter({'Hadoop':doc_frequency['hadoop'], 'MapReduce':doc_frequency['mapreduce'],\n",
    "            'Spark':doc_frequency['spark'], 'Pig':doc_frequency['pig'],\n",
    "            'Hive':doc_frequency['hive'], 'Shark':doc_frequency['shark'],\n",
    "            'Oozie':doc_frequency['oozie'], 'ZooKeeper':doc_frequency['zookeeper'],\n",
    "            'Flume':doc_frequency['flume'], 'Mahout':doc_frequency['mahout']})\n",
    "\n",
    "database_dict = Counter({'SQL':doc_frequency['sql'], 'NoSQL':doc_frequency['nosql'],\n",
    "                'HBase':doc_frequency['hbase'], 'Cassandra':doc_frequency['cassandra'],\n",
    "                'MongoDB':doc_frequency['mongodb']})\n",
    "\n",
    "\n",
    "overall_total_skills = prog_lang_dict + analysis_tool_dict + hadoop_dict + database_dict # Combine our Counter objects\n",
    "\n",
    "\n",
    "\n",
    "final_frame = pd.DataFrame(overall_total_skills.items(), columns = ['Term', 'NumPostings']) # Convert these terms to a \n",
    "                                                                                            # dataframe \n",
    "\n",
    "# Change the values to reflect a percentage of the postings \n",
    "\n",
    "final_frame.NumPostings = (final_frame.NumPostings)*100/len(job_descriptions) # Gives percentage of job postings \n",
    "                                                                                #  having that term \n",
    "\n",
    "# Sort the data for plotting purposes\n",
    "\n",
    "final_frame.sort(columns = 'NumPostings', ascending = False, inplace = True)\n",
    "\n",
    "# Get it ready for a bar plot\n",
    "\n",
    "final_plot = final_frame.plot(x = 'Term', kind = 'bar', legend = None, \n",
    "                        title = 'Percentage of Data Scientist Job Ads with a Key Skill, ' + city_title)\n",
    "\n",
    "final_plot.set_ylabel('Percentage Appearing in Job Ads')\n",
    "fig = final_plot.get_figure() # Have to convert the pandas plot object to a matplotlib object\n",
    "\n",
    "\n",
    "#return fig, final_frame # End of the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
